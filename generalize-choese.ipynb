{"cells":[{"cell_type":"markdown","metadata":{"id":"6f3fsn18kLTB","papermill":{"duration":0.010643,"end_time":"2022-12-21T01:08:29.557950","exception":false,"start_time":"2022-12-21T01:08:29.547307","status":"completed"},"tags":[]},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tranngocduvnvp/CTCDFormer/blob/main/CTDCFormerB4.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:45:56.882528Z","iopub.status.busy":"2023-01-18T12:45:56.881825Z","iopub.status.idle":"2023-01-18T12:46:11.658187Z","shell.execute_reply":"2023-01-18T12:46:11.657137Z","shell.execute_reply.started":"2023-01-18T12:45:56.882417Z"},"id":"vKnxAEbb4vo7","outputId":"931e9cb5-3fea-49b9-a891-0c71fce08e65","papermill":{"duration":12.839045,"end_time":"2022-12-21T01:08:42.406289","exception":false,"start_time":"2022-12-21T01:08:29.567244","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:46:11.662054Z","iopub.status.busy":"2023-01-18T12:46:11.661738Z","iopub.status.idle":"2023-01-18T12:46:21.592517Z","shell.execute_reply":"2023-01-18T12:46:21.591415Z","shell.execute_reply.started":"2023-01-18T12:46:11.662026Z"},"id":"GR0N1x9yDs6G","outputId":"66425beb-bfe8-423d-b55b-663d31424ab6","papermill":{"duration":10.601966,"end_time":"2022-12-21T01:08:53.019161","exception":false,"start_time":"2022-12-21T01:08:42.417195","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install torchgeometry"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:46:21.596591Z","iopub.status.busy":"2023-01-18T12:46:21.596266Z","iopub.status.idle":"2023-01-18T12:46:31.132696Z","shell.execute_reply":"2023-01-18T12:46:31.131461Z","shell.execute_reply.started":"2023-01-18T12:46:21.596563Z"},"id":"fP9t9fQ446Jm","outputId":"034d6eeb-97fe-4e92-c6f0-1feff223d9cb","papermill":{"duration":9.737912,"end_time":"2022-12-21T01:09:02.769811","exception":false,"start_time":"2022-12-21T01:08:53.031899","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:46:31.136560Z","iopub.status.busy":"2023-01-18T12:46:31.135820Z","iopub.status.idle":"2023-01-18T12:46:41.748487Z","shell.execute_reply":"2023-01-18T12:46:41.747131Z","shell.execute_reply.started":"2023-01-18T12:46:31.136518Z"},"id":"t2eIKy8oM4gU","outputId":"78495ba2-e91d-486b-e9b6-1370057ce3be","papermill":{"duration":10.891759,"end_time":"2022-12-21T01:09:13.673967","exception":false,"start_time":"2022-12-21T01:09:02.782208","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip3 install --upgrade gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:46:41.751091Z","iopub.status.busy":"2023-01-18T12:46:41.750687Z","iopub.status.idle":"2023-01-18T12:46:48.806098Z","shell.execute_reply":"2023-01-18T12:46:48.804824Z","shell.execute_reply.started":"2023-01-18T12:46:41.751050Z"},"id":"ngXq172EEgXu","outputId":"bbbc8f25-6c5e-44ed-d890-c3851ce7df4a","papermill":{"duration":34.878693,"end_time":"2022-12-21T01:09:48.564332","exception":false,"start_time":"2022-12-21T01:09:13.685639","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!gdown 1wKiVxrOWtn2JjTMXNoED2FGsT98AMKW6"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:48:34.996833Z","iopub.status.busy":"2023-01-18T12:48:34.996415Z","iopub.status.idle":"2023-01-18T12:48:39.321201Z","shell.execute_reply":"2023-01-18T12:48:39.320039Z","shell.execute_reply.started":"2023-01-18T12:48:34.996800Z"},"id":"HLlUGUUfMG9h","papermill":{"duration":8.298948,"end_time":"2022-12-21T01:09:56.884141","exception":false,"start_time":"2022-12-21T01:09:48.585193","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["!unzip /kaggle/working/data_new.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:48:53.056795Z","iopub.status.busy":"2023-01-18T12:48:53.056405Z","iopub.status.idle":"2023-01-18T12:48:59.672155Z","shell.execute_reply":"2023-01-18T12:48:59.670503Z","shell.execute_reply.started":"2023-01-18T12:48:53.056764Z"},"id":"qOR3AUMN8sz2","outputId":"3eb2fd29-c8be-4412-f6de-a981d49ce258","papermill":{"duration":127.297421,"end_time":"2022-12-21T01:12:04.214574","exception":false,"start_time":"2022-12-21T01:09:56.917153","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!wget https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b2.pth\n","!wget https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b3.pth\n","!wget https://github.com/whai362/PVT/releases/download/v2/pvt_v2_b4.pth"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:48:59.680365Z","iopub.status.busy":"2023-01-18T12:48:59.676093Z","iopub.status.idle":"2023-01-18T12:48:59.701343Z","shell.execute_reply":"2023-01-18T12:48:59.699854Z","shell.execute_reply.started":"2023-01-18T12:48:59.680308Z"},"id":"iJQo-R7N4vo_","papermill":{"duration":4.856272,"end_time":"2022-12-21T01:12:09.136484","exception":false,"start_time":"2022-12-21T01:12:04.280212","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import random\n","import cv2\n","import torch\n","from torch.utils import data\n","import torchvision.transforms.functional as TF\n","import numpy as np\n","import random\n","import multiprocessing\n","import timm\n","import torchmetrics\n","import sys\n","import os\n","import argparse\n","import time\n","import numpy as np\n","import glob\n","\n","import torch.nn as nn\n","import torchmetrics.functional.classification as Fmstric\n","import torchgeometry as tgm\n","\n","\n","import torch.nn.functional as F\n","from functools import partial\n","from timm.models.vision_transformer import _cfg\n","from texttable import Texttable\n","\n","\n","from sklearn.model_selection import train_test_split\n","from torchvision import transforms\n","import glob\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:08.480239Z","iopub.status.busy":"2023-01-18T12:52:08.479773Z","iopub.status.idle":"2023-01-18T12:52:08.493805Z","shell.execute_reply":"2023-01-18T12:52:08.492642Z","shell.execute_reply.started":"2023-01-18T12:52:08.480201Z"},"id":"tBmoDkE84vpA","outputId":"f7b3417c-1000-48df-c0bc-aab994078332","papermill":{"duration":0.115313,"end_time":"2022-12-21T01:12:09.369476","exception":false,"start_time":"2022-12-21T01:12:09.254163","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class Args:\n","    def __init__(self,root, epochs, batch_size, dataset, mgpu, lrs_min,\\\n","                 lrs, lr, type_lr, checkpoint_path, backbone, optim):\n","        self.root = root\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.dataset = dataset\n","        self.mgpu = mgpu\n","        self.lrs_min = lrs_min\n","        self.lrs = lrs\n","        self.lr = lr\n","        self.type_lr = type_lr\n","        self.checkpoint_path = checkpoint_path\n","        self.backbone = backbone\n","        self.optim = optim\n","        \n","args = Args(\n","    root=\"/kaggle/working/data_new\", \n","    epochs=80, \n","    batch_size=4, \n","    dataset=\"generalizability_20_12\",\n","    mgpu=\"false\",\n","    lrs=\"true\",\n","    lrs_min=1e-6,\n","    lr = 1e-4,\n","    type_lr = \"StepLR\",\n","    checkpoint_path = None,\n","    backbone=\"PvtB3\",\n","    optim=\"AdamW\"\n",")\n","args.root"]},{"cell_type":"markdown","metadata":{"id":"YXobI07G4vpB","papermill":{"duration":0.111726,"end_time":"2022-12-21T01:12:09.589827","exception":false,"start_time":"2022-12-21T01:12:09.478101","status":"completed"},"tags":[]},"source":["# Make Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:15.370794Z","iopub.status.busy":"2023-01-18T12:52:15.370431Z","iopub.status.idle":"2023-01-18T12:52:15.383276Z","shell.execute_reply":"2023-01-18T12:52:15.382185Z","shell.execute_reply.started":"2023-01-18T12:52:15.370765Z"},"id":"s3GSDxe34vpD","papermill":{"duration":0.115216,"end_time":"2022-12-21T01:12:09.798794","exception":false,"start_time":"2022-12-21T01:12:09.683578","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class SegDataset(data.Dataset):\n","    def __init__(\n","        self,\n","        input_paths: list,\n","        target_paths: list,\n","        transform_input=None,\n","        transform_target=None,\n","        hflip=False,\n","        vflip=False,\n","        affine=False,\n","    ):\n","        self.input_paths = input_paths\n","        self.target_paths = target_paths\n","        self.transform_input = transform_input\n","        self.transform_target = transform_target\n","        self.hflip = hflip\n","        self.vflip = vflip\n","        self.affine = affine\n","\n","    def __len__(self):\n","        return len(self.input_paths)\n","\n","    def __getitem__(self, index: int):\n","        input_ID = self.input_paths[index]\n","        target_ID = self.target_paths[index]\n","\n","        x = cv2.cvtColor(cv2.imread(input_ID), cv2.COLOR_BGR2RGB)\n","        y = cv2.cvtColor(cv2.imread(target_ID), cv2.COLOR_BGR2RGB)\n","        x = self.transform_input(x)\n","        y = self.transform_target(y)\n","\n","        if self.hflip:\n","            if random.uniform(0.0, 1.0) > 0.5:\n","                x = TF.hflip(x)\n","                y = TF.hflip(y)\n","\n","        if self.vflip:\n","            if random.uniform(0.0, 1.0) > 0.5:\n","                x = TF.vflip(x)\n","                y = TF.vflip(y)\n","\n","        if self.affine:\n","            angle = random.uniform(-180.0, 180.0)\n","            h_trans = random.uniform(-352 / 8, 352 / 8)\n","            v_trans = random.uniform(-352 / 8, 352 / 8)\n","            scale = random.uniform(0.5, 1.5)\n","            shear = random.uniform(-22.5, 22.5)\n","            x = TF.affine(x, angle, (h_trans, v_trans), scale, shear, fill=-1.0)\n","            y = TF.affine(y, angle, (h_trans, v_trans), scale, shear, fill=0.0)\n","        return x.float(), y.float()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpOv-X1-4vpF","papermill":{"duration":3.847189,"end_time":"2022-12-21T01:12:13.748084","exception":false,"start_time":"2022-12-21T01:12:09.900895","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def split_ids(len_ids):\n","    train_size = int(round((80 / 100) * len_ids))\n","    valid_size = int(round((10 / 100) * len_ids))\n","    test_size = int(round((10 / 100) * len_ids))\n","\n","    train_indices, test_indices = train_test_split(\n","        np.linspace(0, len_ids - 1, len_ids).astype(\"int\"),\n","        test_size=test_size,\n","        random_state=42,\n","    )\n","\n","    train_indices, val_indices = train_test_split(\n","        train_indices, test_size=test_size, random_state=42\n","    )\n","\n","    return train_indices, test_indices, val_indices\n","\n","\n","def get_dataloaders(root_train, root_test, batch_size):\n","    input_paths_train = sorted(glob.glob(root_train + \"/images/*\"))\n","    target_path_train = sorted(glob.glob(root_train + \"/masks/*\"))\n","    dataset_test = [\"Kvasir\", \"ETIS-LaribPolypDB\", \"CVC-ColonDB\", \"CVC-ClinicDB\",\"CVC-300\"]\n","    path_image_test = {}\n","    path_mask_test = {}\n","    for item_name in dataset_test:\n","        path_image_test[item_name] = sorted(glob.glob(root_test + f\"/{item_name}\" + \"/images/*\"))\n","        path_mask_test[item_name] = sorted(glob.glob(root_test + f\"/{item_name}\" + \"/masks/*\"))\n","\n","    transform_input4train = transforms.Compose(\n","        [\n","            transforms.ToTensor(),\n","            transforms.Resize((352, 352), antialias=True),\n","            transforms.GaussianBlur((25, 25), sigma=(0.001, 2.0)),\n","            transforms.ColorJitter(\n","                brightness=0.4, contrast=0.5, saturation=0.25, hue=0.01\n","            ),\n","            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5]),\n","        ]\n","    )\n","\n","    transform_input4test = transforms.Compose(\n","        [\n","            transforms.ToTensor(),\n","            transforms.Resize((352, 352), antialias=True),\n","            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5]),\n","        ]\n","    )\n","\n","    transform_target = transforms.Compose(\n","        [transforms.ToTensor(), transforms.Resize((352, 352)), transforms.Grayscale()]\n","    )\n","\n","    train_dataset = SegDataset(\n","        input_paths=input_paths_train,\n","        target_paths=target_path_train,\n","        transform_input=transform_input4train,\n","        transform_target=transform_target,\n","        hflip=True,\n","        vflip=True,\n","        affine=True,\n","    )\n","    test_dataset = {}\n","    for item in path_image_test:\n","      test_dataset[item] = SegDataset(\n","        input_paths=path_image_test[item],\n","        target_paths=path_mask_test[item],\n","        transform_input=transform_input4test,\n","        transform_target=transform_target,)\n","\n","    \n","    train_dataloader = data.DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        drop_last=True,\n","        num_workers=multiprocessing.Pool()._processes,\n","    )\n","    test_dataloader = {}\n","    for item in test_dataset:\n","      test_dataloader[item] = data.DataLoader(\n","        dataset=test_dataset[item],\n","        batch_size=1,\n","        shuffle=False,\n","        num_workers=multiprocessing.Pool()._processes,)\n","\n","\n","\n","    return train_dataloader, test_dataloader\n","\n","\n","\n","root_train = \"/kaggle/working/data_new/TrainDataset\"\n","root_test = \"/kaggle/working/data_new/TestDataset\"\n","train_dataloader, test_dataloader = get_dataloaders(\n","    root_train,\n","    root_test,\n","    4\n",")\n","        \n","image, mask = next(iter(train_dataloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:52:15.492375Z","iopub.status.idle":"2023-01-18T12:52:15.493667Z","shell.execute_reply":"2023-01-18T12:52:15.493421Z","shell.execute_reply.started":"2023-01-18T12:52:15.493395Z"},"id":"0cRw35_oabyY","papermill":{"duration":0.19377,"end_time":"2022-12-21T01:12:14.034924","exception":false,"start_time":"2022-12-21T01:12:13.841154","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def show(img, mask):\n","    plt.subplot(1,2,1)\n","    plt.imshow(img[0].permute(1,2,0))\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.subplot(1,2,2)\n","    plt.imshow(mask[0][0], cmap=\"gray\")\n","    plt.xticks([])\n","    plt.yticks([])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:52:15.495370Z","iopub.status.idle":"2023-01-18T12:52:15.495876Z","shell.execute_reply":"2023-01-18T12:52:15.495638Z","shell.execute_reply.started":"2023-01-18T12:52:15.495614Z"},"id":"Qr3WFOsOaHJH","outputId":"4fc5eb41-d6b6-48fb-c590-93f3bcea54f3","papermill":{"duration":0.788602,"end_time":"2022-12-21T01:12:14.892760","exception":false,"start_time":"2022-12-21T01:12:14.104158","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["image, mask = next(iter(test_dataloader[\"ETIS-LaribPolypDB\"])) \n","show(image, mask)"]},{"cell_type":"markdown","metadata":{"id":"WrpFsb-E4vpH","papermill":{"duration":0.062283,"end_time":"2022-12-21T01:12:15.041138","exception":false,"start_time":"2022-12-21T01:12:14.978855","status":"completed"},"tags":[]},"source":["# make model"]},{"cell_type":"markdown","metadata":{"id":"2qtOlRLS4vpI","papermill":{"duration":0.094451,"end_time":"2022-12-21T01:12:15.195241","exception":false,"start_time":"2022-12-21T01:12:15.100790","status":"completed"},"tags":[]},"source":["## Backbone"]},{"cell_type":"markdown","metadata":{"id":"YoZzR-Ru6OfG","papermill":{"duration":0.062869,"end_time":"2022-12-21T01:12:15.322986","exception":false,"start_time":"2022-12-21T01:12:15.260117","status":"completed"},"tags":[]},"source":["## MIT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:15.503207Z","iopub.status.busy":"2023-01-18T12:52:15.502909Z","iopub.status.idle":"2023-01-18T12:52:15.586949Z","shell.execute_reply":"2023-01-18T12:52:15.585914Z","shell.execute_reply.started":"2023-01-18T12:52:15.503182Z"},"id":"obL-69IW4vpJ","papermill":{"duration":0.14955,"end_time":"2022-12-21T01:12:15.532995","exception":false,"start_time":"2022-12-21T01:12:15.383445","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import warnings\n","\n","from functools import partial\n","from timm.models.layers import to_2tuple, trunc_normal_\n","import math\n","from timm.models.layers import DropPath\n","import torch.nn as nn\n","import torch\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.dwconv = DWConv(hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = self.fc1(x)\n","        x = self.dwconv(x, H, W)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n","        super().__init__()\n","        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n","\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        self.sr_ratio = sr_ratio\n","        if sr_ratio > 1:\n","            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n","            self.norm = nn.LayerNorm(dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","\n","        if self.sr_ratio > 1:\n","            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n","            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n","            x_ = self.norm(x_)\n","            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        else:\n","            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        k, v = kv[0], kv[1]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim,\n","            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","    def forward(self, x, H, W):\n","        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n","        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n","\n","        return x\n","\n","\n","class OverlapPatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n","        self.num_patches = self.H * self.W\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n","                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        x = self.proj(x)\n","        _, _, H, W = x.shape\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","\n","        return x, H, W\n","\n","\n","class MixVisionTransformer(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n","                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n","                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n","                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.depths = depths\n","        self.embed_dims = embed_dims\n","\n","        # patch_embed\n","        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n","                                              embed_dim=embed_dims[0])\n","        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n","                                              embed_dim=embed_dims[1])\n","        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n","                                              embed_dim=embed_dims[2])\n","        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n","                                              embed_dim=embed_dims[3])\n","\n","        # transformer encoder\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","        cur = 0\n","        self.block1 = nn.ModuleList([Block(\n","            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[0])\n","            for i in range(depths[0])])\n","        self.norm1 = norm_layer(embed_dims[0])\n","\n","        cur += depths[0]\n","        self.block2 = nn.ModuleList([Block(\n","            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[1])\n","            for i in range(depths[1])])\n","        self.norm2 = norm_layer(embed_dims[1])\n","\n","        cur += depths[1]\n","        self.block3 = nn.ModuleList([Block(\n","            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[2])\n","            for i in range(depths[2])])\n","        self.norm3 = norm_layer(embed_dims[2])\n","\n","        cur += depths[2]\n","        self.block4 = nn.ModuleList([Block(\n","            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[3])\n","            for i in range(depths[3])])\n","        self.norm4 = norm_layer(embed_dims[3])\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        outs = []\n","\n","        # stage 1\n","        x, H, W = self.patch_embed1(x)\n","        for i, blk in enumerate(self.block1):\n","            x = blk(x, H, W)\n","        x = self.norm1(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 2\n","        x, H, W = self.patch_embed2(x)\n","        for i, blk in enumerate(self.block2):\n","            x = blk(x, H, W)\n","        x = self.norm2(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 3\n","        x, H, W = self.patch_embed3(x)\n","        for i, blk in enumerate(self.block3):\n","            x = blk(x, H, W)\n","        x = self.norm3(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 4\n","        x, H, W = self.patch_embed4(x)\n","        for i, blk in enumerate(self.block4):\n","            x = blk(x, H, W)\n","        x = self.norm4(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        return outs\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","\n","        return x\n","\n","\n","class DWConv(nn.Module):\n","    def __init__(self, dim=768):\n","        super(DWConv, self).__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        x = x.transpose(1, 2).view(B, C, H, W)\n","        x = self.dwconv(x)\n","        x = x.flatten(2).transpose(1, 2)\n","\n","        return x\n","\n","\n","class mit_b0(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b0, self).__init__(\n","            patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b1(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b1, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b2(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b2, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b3(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b3, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b4(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b4, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b5(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b5, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","        \n","class Feature_extractor_MIT(nn.Module):\n","\n","    def __init__(self,model_type=\"MitB0\", embedding_dim = 160):\n","        super(Feature_extractor_MIT, self).__init__()\n","        self.model_type = model_type\n","        # Backbone\n","        if self.model_type == 'MitB0':\n","            self.backbone = mit_b0()\n","            self.channel = [32, 64, 160, 256]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if self.model_type == 'MitB1':\n","            self.backbone = mit_b1()\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if self.model_type == 'MitB2':\n","            self.backbone = mit_b2()\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if self.model_type == 'MitB3':\n","            self.backbone = mit_b3()\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if self.model_type == 'MitB4':\n","            self.backbone = mit_b4()\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if self.model_type == 'MitB5':\n","            self.backbone = mit_b5()\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        self._init_weights()  # load pretrain\n","        \n","        \n","    def _init_weights(self):\n","        \n","        if self.model_type == 'MitB0':\n","            pretrained_dict = torch.load('./Pretrained/mit_b0.pth')\n","        if self.model_type == 'MitB1':\n","            pretrained_dict = torch.load('./Pretrained/mit_b1.pth')\n","        if self.model_type == 'MitB2':\n","            pretrained_dict = torch.load('./Pretrained/mit_b2.pth')\n","        if self.model_type == 'MitB3':\n","            pretrained_dict = torch.load('./Pretrained/mit_b3.pth')\n","        if self.model_type == 'MitB4':\n","            pretrained_dict = torch.load('./Pretrained/mit_b4.pth')\n","        if self.model_type == 'MitB5':\n","            pretrained_dict = torch.load('./Pretrained/mit_b5.pth')\n","            \n","            \n","        model_dict = self.backbone.state_dict()\n","        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n","        model_dict.update(pretrained_dict)\n","        self.backbone.load_state_dict(model_dict)\n","        print(\"successfully loaded!!!!\")\n","        \n","        \n","    def forward(self, x):\n","        \n","        ##################  Go through backbone ###################\n","        \n","        B = x.shape[0]\n","        \n","        #stage 1\n","        out_1, H, W = self.backbone.patch_embed1(x)\n","        for i, blk in enumerate(self.backbone.block1):\n","            out_1 = blk(out_1, H, W)\n","        out_1 = self.backbone.norm1(out_1)\n","        out_1 = out_1.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[0], 88, 88)\n","        \n","        # stage 2\n","        out_2, H, W = self.backbone.patch_embed2(out_1)\n","        for i, blk in enumerate(self.backbone.block2):\n","            out_2 = blk(out_2, H, W)\n","        out_2 = self.backbone.norm2(out_2)\n","        out_2 = out_2.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[1], 44, 44)\n","        \n","        # stage 3\n","        out_3, H, W = self.backbone.patch_embed3(out_2)\n","        for i, blk in enumerate(self.backbone.block3):\n","            out_3 = blk(out_3, H, W)\n","        out_3 = self.backbone.norm3(out_3)\n","        out_3 = out_3.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[2], 22, 22)\n","        \n","        # stage 4\n","        out_4, H, W = self.backbone.patch_embed4(out_3)\n","        for i, blk in enumerate(self.backbone.block4):\n","            out_4 = blk(out_4, H, W)\n","        out_4 = self.backbone.norm4(out_4)\n","        out_4 = out_4.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[3], 11, 11)\n","        \n","        return out_1, out_2, out_3, out_4\n","    \n","# model = Feature_extractor_MIT(model_type = \"MitB5\")\n","# x = torch.rand(2,3,352,352)\n","# outs = model(x)\n","# channels = []\n","# for out in outs:\n","#     print(out.shape)\n","#     channels.append(out.shape[1])\n","# print(channels)"]},{"cell_type":"markdown","metadata":{"id":"VYstq8pi6V1p","papermill":{"duration":0.059574,"end_time":"2022-12-21T01:12:15.651192","exception":false,"start_time":"2022-12-21T01:12:15.591618","status":"completed"},"tags":[]},"source":["##Pvit"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:15.607739Z","iopub.status.busy":"2023-01-18T12:52:15.607484Z","iopub.status.idle":"2023-01-18T12:52:20.756454Z","shell.execute_reply":"2023-01-18T12:52:20.755378Z","shell.execute_reply.started":"2023-01-18T12:52:15.607715Z"},"id":"JshwNWTF6LXk","outputId":"92849f7b-631e-47bb-dfe7-5f2c41210f4e","papermill":{"duration":4.648811,"end_time":"2022-12-21T01:12:20.359663","exception":false,"start_time":"2022-12-21T01:12:15.710852","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","#Ported from https://github.com/whai362/PVT (unmodified)\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from functools import partial\n","\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.registry import register_model\n","from timm.models.vision_transformer import _cfg\n","import math\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., linear=False):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.dwconv = DWConv(hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","        self.linear = linear\n","        if self.linear:\n","            self.relu = nn.ReLU(inplace=True)\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = self.fc1(x)\n","        if self.linear:\n","            x = self.relu(x)\n","        x = self.dwconv(x, H, W)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1, linear=False):\n","        super().__init__()\n","        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n","\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        self.linear = linear\n","        self.sr_ratio = sr_ratio\n","        if not linear:\n","            if sr_ratio > 1:\n","                self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n","                self.norm = nn.LayerNorm(dim)\n","        else:\n","            self.pool = nn.AdaptiveAvgPool2d(7)\n","            self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1)\n","            self.norm = nn.LayerNorm(dim)\n","            self.act = nn.GELU()\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","\n","        if not self.linear:\n","            if self.sr_ratio > 1:\n","                x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n","                x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n","                x_ = self.norm(x_)\n","                kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","            else:\n","                kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        else:\n","            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n","            x_ = self.sr(self.pool(x_)).reshape(B, C, -1).permute(0, 2, 1)\n","            x_ = self.norm(x_)\n","            x_ = self.act(x_)\n","            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        k, v = kv[0], kv[1]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, linear=False):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim,\n","            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio, linear=linear)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, linear=linear)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n","        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n","\n","        return x\n","\n","\n","class OverlapPatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        \n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        \n","        assert max(patch_size) > stride, \"Set larger patch_size than stride\"\n","        \n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.H, self.W = img_size[0] // stride, img_size[1] // stride\n","        self.num_patches = self.H * self.W\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n","                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.proj(x)\n","        _, _, H, W = x.shape\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","\n","        return x, H, W\n","\n","\n","class PyramidVisionTransformerV2(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n","                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n","                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n","                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], num_stages=4, linear=False):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.depths = depths\n","        self.num_stages = num_stages\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","        cur = 0\n","\n","        for i in range(num_stages):\n","            patch_embed = OverlapPatchEmbed(img_size=img_size if i == 0 else img_size // (2 ** (i + 1)),\n","                                            patch_size=7 if i == 0 else 3,\n","                                            stride=4 if i == 0 else 2,\n","                                            in_chans=in_chans if i == 0 else embed_dims[i - 1],\n","                                            embed_dim=embed_dims[i])\n","\n","            block = nn.ModuleList([Block(\n","                dim=embed_dims[i], num_heads=num_heads[i], mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer,\n","                sr_ratio=sr_ratios[i], linear=linear)\n","                for j in range(depths[i])])\n","            norm = norm_layer(embed_dims[i])\n","            cur += depths[i]\n","\n","            setattr(self, f\"patch_embed{i + 1}\", patch_embed)\n","            setattr(self, f\"block{i + 1}\", block)\n","            setattr(self, f\"norm{i + 1}\", norm)\n","\n","        # classification head\n","        self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def freeze_patch_emb(self):\n","        self.patch_embed1.requires_grad = False\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        features_maps = []\n","\n","        for i in range(self.num_stages):\n","            patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n","            block = getattr(self, f\"block{i + 1}\")\n","            norm = getattr(self, f\"norm{i + 1}\")\n","            x, H, W = patch_embed(x)\n","            for blk in block:\n","                x = blk(x, H, W)\n","            x = norm(x)\n","            features_maps.append(x.view(B, H, W, -1).permute(0, 3, 1, 2).contiguous())\n","            if i != self.num_stages - 1:\n","                x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","                # print(i, x.shape)\n","\n","        return features_maps\n","\n","    def forward(self, x):\n","        # print(x.shape)\n","        x = self.forward_features(x)\n","\n","        return x\n","\n","\n","class DWConv(nn.Module):\n","    def __init__(self, dim=768):\n","        super(DWConv, self).__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        x = x.transpose(1, 2).view(B, C, H, W)\n","        x = self.dwconv(x)\n","        x = x.flatten(2).transpose(1, 2)\n","\n","        return x\n","\n","\n","def _conv_filter(state_dict, patch_size=16):\n","    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n","    out_dict = {}\n","    for k, v in state_dict.items():\n","        if 'patch_embed.proj.weight' in k:\n","            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n","        out_dict[k] = v\n","\n","    return out_dict\n","\n","\n","@register_model\n","def pvt_v2_b0(pretrained=False, **kwargs):\n","    model = PyramidVisionTransformerV2(\n","        patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","\n","    return model\n","\n","\n","@register_model\n","def pvt_v2_b1(pretrained=False, **kwargs):\n","    model = PyramidVisionTransformerV2(\n","        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","\n","    return model\n","\n","\n","@register_model\n","def pvt_v2_b2(pretrained=False, **kwargs):\n","    model = PyramidVisionTransformerV2(\n","        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n","    model.default_cfg = _cfg()\n","\n","    return model\n","\n","\n","@register_model\n","def pvt_v2_b3(pretrained=False, **kwargs):\n","    model = PyramidVisionTransformerV2(\n","        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","\n","    return model\n","\n","\n","@register_model\n","def pvt_v2_b4(pretrained=False, **kwargs):\n","    model = PyramidVisionTransformerV2(\n","        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","\n","    return model\n","\n","\n","@register_model\n","def pvt_v2_b5(pretrained=False, **kwargs):\n","    model = PyramidVisionTransformerV2(\n","        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","\n","    return model\n","\n","\n","@register_model\n","def pvt_v2_b2_li(pretrained=False, **kwargs):\n","    model = PyramidVisionTransformerV2(\n","        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], linear=True, **kwargs)\n","    model.default_cfg = _cfg()\n","\n","    return model\n","\n","class Feature_extractor_PVT(nn.Module):\n","    def __init__(self, model_type=\"PvtB4\"):\n","        super().__init__()\n","        if model_type == 'PvtB0':\n","            pretrained_dict = torch.load('./pvt_v2_b0.pth')\n","            self.backbone = pvt_v2_b0()\n","            self.backbone.load_state_dict(pretrained_dict)\n","            print(\"Load checkpoint successfuly!!\")\n","            self.channel = [32, 64, 160, 256]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if model_type == 'PvtB1':\n","            pretrained_dict = torch.load('./pvt_v2_b1.pth')\n","            self.backbone = pvt_v2_b1()\n","            self.backbone.load_state_dict(pretrained_dict)\n","            print(\"Load checkpoint successfuly!!\")\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if model_type == 'PvtB2':\n","            pretrained_dict = torch.load('./pvt_v2_b2.pth')\n","            self.backbone = pvt_v2_b2()\n","            self.backbone.load_state_dict(pretrained_dict)\n","            print(\"Load checkpoint successfuly!!\")\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if model_type == 'PvtB3':\n","            pretrained_dict = torch.load('./pvt_v2_b3.pth')\n","            self.backbone = pvt_v2_b3()\n","            self.backbone.load_state_dict(pretrained_dict)\n","            print(\"Load checkpoint successfuly!!\")\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","        if model_type == 'PvtB4':\n","            pretrained_dict = torch.load('./pvt_v2_b4.pth')\n","            self.backbone = pvt_v2_b4()\n","            self.backbone.load_state_dict(pretrained_dict)\n","            print(\"Load checkpoint successfuly!!\")\n","            self.channel = [64, 128, 320, 512]\n","            self.scale = [1/8, 1/4, 1/2, 1/1, 2, 4, 8]\n","\n","\n","    def forward(self, x):\n","        features = self.backbone(x)\n","\n","        return features\n","\n","x = torch.rand(2,3,224,224)\n","model = Feature_extractor_PVT()\n","out = model(x)\n","for item in out:\n","    print(item.shape)\n","# print(out.shape)"]},{"cell_type":"markdown","metadata":{"id":"JOfhbVKc4vpN","papermill":{"duration":0.061397,"end_time":"2022-12-21T01:12:20.483530","exception":false,"start_time":"2022-12-21T01:12:20.422133","status":"completed"},"tags":[]},"source":["## model"]},{"cell_type":"markdown","metadata":{"id":"SyUuvwASsYMl","papermill":{"duration":0.063994,"end_time":"2022-12-21T01:12:20.609612","exception":false,"start_time":"2022-12-21T01:12:20.545618","status":"completed"},"tags":[]},"source":["## Module"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:20.760581Z","iopub.status.busy":"2023-01-18T12:52:20.760287Z","iopub.status.idle":"2023-01-18T12:52:20.794324Z","shell.execute_reply":"2023-01-18T12:52:20.793366Z","shell.execute_reply.started":"2023-01-18T12:52:20.760554Z"},"id":"d3XwCBoQk_Yt","papermill":{"duration":0.100802,"end_time":"2022-12-21T01:12:20.775004","exception":false,"start_time":"2022-12-21T01:12:20.674202","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class EFFat(nn.Module):\n","\n","    def __init__(self, channel=512,reduction=16):\n","        super().__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, channel // reduction, bias=False),\n","            nn.GELU(),\n","            nn.Linear(channel // reduction, channel, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        b, c, _, _ = x.size()\n","        y = self.avg_pool(x).view(b, c) + self.max_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","        return x * y.expand_as(x) + x\n","    \n","\n","class convMixerLayer(nn.Module):\n","    def __init__(self, in_channel, out_channel, kernel_size) -> None:\n","        super().__init__()\n","        self.depthwise = nn.Conv2d(\n","            in_channels=in_channel, \n","            out_channels=in_channel, \n","            groups=in_channel, \n","            kernel_size=kernel_size,\n","            stride=1,\n","            padding=\"same\"\n","        )\n","        self.eff = EFFat(out_channel)\n","        self.pointwise = nn.Conv2d(in_channel, out_channel, 1, 1)\n","        self.activation = nn.SiLU()\n","        self.batchnorm1 = nn.BatchNorm2d(in_channel)\n","        self.batchnorm2 = nn.BatchNorm2d(out_channel)\n","    \n","    def forward(self, x):\n","        ori = x\n","        x = self.depthwise(x)\n","        x = self.activation(x)\n","        x = self.eff(self.batchnorm1(x)) + ori\n","        x = self.pointwise(x)\n","        x = self.activation(x)\n","        x = self.batchnorm2(x)\n","        return x\n","\n","\n","class SEModule(nn.Module):\n","    def __init__(self, channels, reduction):\n","        super(SEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(\n","            channels, channels // reduction, kernel_size=1, padding=0 ,bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(\n","            channels // reduction, channels, kernel_size=1, padding=0 ,bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        module_input = x\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return module_input * x\n","\n","def autopad(k, p=None, d=1):  # kernel, padding, dilation\n","    # Pad to 'same' shape outputs\n","    if d > 1:\n","        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n","    if p is None:\n","        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n","    return p\n","\n","class Conv(nn.Module):\n","    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)\n","    default_act = nn.SiLU()  # default activation\n","\n","    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n","        super().__init__()\n","        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n","        self.bn = nn.BatchNorm2d(c2)\n","        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n","\n","    def forward(self, x):\n","        return self.act(self.bn(self.conv(x)))\n","\n","    def forward_fuse(self, x):\n","        return self.act(self.conv(x))\n","\n","class Bottleneck(nn.Module):\n","    # Standard bottleneck\n","    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n","        super().__init__()\n","        c_ = int(c2 * e)  # hidden channels\n","        self.cv1 = Conv(c1, c_, 1, 1)\n","        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n","        self.add = shortcut and c1 == c2\n","\n","    def forward(self, x):\n","        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n","\n","class BottleneckCSP(nn.Module):\n","    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n","    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n","        super().__init__()\n","        c_ = int(c2 * e)  # hidden channels\n","        self.cv1 = Conv(c1, c_, 1, 1)\n","        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n","        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n","        self.cv4 = Conv(2 * c_, c2, 1, 1)\n","        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n","        self.act = nn.SiLU()\n","        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n","\n","    def forward(self, x):\n","        y1 = self.cv3(self.m(self.cv1(x)))\n","        y2 = self.cv2(x)\n","        return self.cv4(self.act(self.bn(torch.cat((y1, y2), 1))))\n","\n","class Channel_attention(nn.Module):\n","    def __init__(self, c, reduction=16) -> None:\n","        super().__init__()\n","        self.fcap1 = nn.Conv2d(c, c//reduction,1)\n","        self.fcmp1 = nn.Conv2d(c, c//reduction, 1)\n","        self.fcap2 = nn.Conv2d(c//reduction, c, 1)\n","        self.fcmp2 = nn.Conv2d(c//reduction, c, 1)\n","        self.acg = nn.Conv2d(c//reduction, c, 1)\n","    def forward(self, x):\n","        f1 = F.relu(self.fcap1(x))\n","        f2 = F.relu(self.fcmp1(x))\n","        f = self.acg(f1 + f2)\n","        out = F.sigmoid(f + self.fcap2(f1) + self.fcmp2(f2))*x\n","        return out\n","\n","class CTblock(nn.Module):\n","    def __init__(self, in_channel, out_channel, kernel_size=3) -> None:\n","        super().__init__()\n","        self.mixspatial = nn.Sequential(\n","            nn.Conv2d(in_channel, in_channel, kernel_size, padding=\"same\", groups=in_channel),\n","            nn.BatchNorm2d(in_channel),\n","            nn.ReLU(),\n","            Channel_attention(in_channel),\n","            nn.Conv2d(in_channel, out_channel, 3, padding=\"same\"),\n","            nn.BatchNorm2d(out_channel),\n","        )\n","        self.mixchannel = nn.Sequential(\n","            nn.Conv2d(in_channel, out_channel, 1, 1),\n","            nn.BatchNorm2d(out_channel),\n","        )\n","    \n","    def forward(self, x):\n","        y = F.relu(self.mixspatial(x) + self.mixchannel(x))\n","        return y\n"]},{"cell_type":"markdown","metadata":{"id":"CMCWZ9pPsjlr","papermill":{"duration":0.062327,"end_time":"2022-12-21T01:12:20.900155","exception":false,"start_time":"2022-12-21T01:12:20.837828","status":"completed"},"tags":[]},"source":["## CTDCFormer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:20.798495Z","iopub.status.busy":"2023-01-18T12:52:20.798012Z","iopub.status.idle":"2023-01-18T12:52:26.447882Z","shell.execute_reply":"2023-01-18T12:52:26.446921Z","shell.execute_reply.started":"2023-01-18T12:52:20.798467Z"},"id":"vM_JjVEZ4vpN","outputId":"afcd9490-b7b2-4640-e3f8-9e8c90569101","papermill":{"duration":6.169207,"end_time":"2022-12-21T01:12:27.130656","exception":false,"start_time":"2022-12-21T01:12:20.961449","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class UpDownstream(nn.Module):\n","    def __init__(self, scale, in_channel, out_channel) -> None:\n","        super().__init__()\n","        self.scale = scale\n","        self.conv1x1 = nn.Conv2d(in_channel, out_channel, 1)\n","        self.bn = nn.BatchNorm2d(out_channel)\n","        self.ac = nn.GELU()\n","    def forward(self, x):\n","        x = self.conv1x1(x)\n","        x = self.bn(x)\n","        x = self.ac(x)\n","        bn, c, h, w = x.shape\n","        x = F.interpolate(x, size=(int(h*self.scale), int(w*self.scale)), mode=\"bilinear\")\n","        return x\n","\n","class NormMode(nn.Module):\n","    def __init__(self, scale, in_channel, out_channel) -> None:\n","        super().__init__()\n","        \"\"\"nhn u vo l mt tensor cxhxw\n","\n","        Returns:\n","            - vector key: d\n","            - Tensor value: c1xh1xw1\n","        \"\"\"\n","        self.norm = UpDownstream(scale, in_channel, out_channel)\n","        self.avg = nn.AdaptiveAvgPool2d((1,1))\n","        self.mg = nn.AdaptiveMaxPool2d((1,1))\n","        self.mlp = nn.Sequential(\n","            nn.Conv2d(in_channel, in_channel, 1),\n","            nn.GELU(),\n","            nn.Conv2d(in_channel, out_channel, 1)\n","        )\n","\n","    def forward(self, x):\n","        v = self.norm(x).unsqueeze(1) # (bs, 1, c, h, w)\n","        k = self.mlp(self.avg(x)+self.mg(x)).view(x.shape[0], 1, -1) #(bs,1, c)\n","        return v, k\n","        \n","\n","class AttentionDC(nn.Module):\n","    def __init__(self, scale, in_channel, out_channel) -> None:\n","        super().__init__()\n","        self.normfm1 = NormMode(scale[0], in_channel[0], out_channel)\n","        self.normfm2 = NormMode(scale[1], in_channel[1], out_channel)\n","        self.normfm3 = NormMode(scale[2], in_channel[2], out_channel)\n","        self.normfm4 = NormMode(scale[3], in_channel[3], out_channel)\n","        self.normfmDecode = NormMode(1, out_channel, out_channel)\n","        self.mlp = nn.Linear(out_channel*2, out_channel)\n","\n","    \n","    def forward(self, feature_maps):\n","        fm1, fm2, fm3, fm4, fmdecode = feature_maps\n","        v1, k1 = self.normfm1(fm1)\n","        v2, k2 = self.normfm2(fm2)\n","        v3, k3 = self.normfm3(fm3)\n","        v4, k4 = self.normfm4(fm4)\n","        vd, qd = self.normfmDecode(fmdecode) #(bs, 1, c)\n","        K = torch.cat([k1, k2, k3, k4], dim=1) #(bs, 4, c)\n","        K = torch.cat([K, qd.expand_as(K)], dim=2) #(bs, 4, 2c)\n","        atten = F.softmax(self.mlp(K), dim=1).unsqueeze(-1).unsqueeze(-1)   #(bs, 4, c, 1, 1)\n","        V = torch.cat([v1,v2,v3,v4], dim=1) #(bs, 4, c, h, w)\n","        V = V*atten #(bs, 4, c, h, w)\n","        V = torch.sum(V, dim=1) #(bs, c, h, w)\n","        V = torch.cat([V, vd.squeeze(1)], dim=1)\n","        return V\n","\n","class RB(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        self.in_layers = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, dilation=2, padding=\"same\"),\n","            nn.BatchNorm2d(out_channels),\n","            nn.GELU(),\n","        )\n","\n","        self.out_layers = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, dilation=2, padding=\"same\"),\n","            nn.BatchNorm2d(out_channels, out_channels),\n","            nn.GELU(),\n","        )\n","\n","        if out_channels == in_channels:\n","            self.skip = nn.Identity()\n","        else:\n","            self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        h = self.in_layers(x)\n","        h = self.out_layers(h)\n","        return h + self.skip(x)\n","\n","        \n","\n","class CTDC(nn.Module):\n","    def __init__(self, args = args) -> None:\n","        super().__init__()\n","        if \"Mit\" in args.backbone:\n","            self.feature_extractor = Feature_extractor_MIT(args.backbone)\n","            channel, scale = self.feature_extractor.channel, self.feature_extractor.scale\n","        elif \"Pvt\" in args.backbone:\n","            self.feature_extractor = Feature_extractor_PVT(args.backbone)\n","            channel, scale = self.feature_extractor.channel, self.feature_extractor.scale\n","        \n","        self.attention1 = AttentionDC(scale[:4], channel, channel[-1])\n","        self.attention2 = AttentionDC(scale[1:5], channel, channel[-2])\n","        self.attention3 = AttentionDC(scale[2:6], channel, channel[-3])\n","        self.attention4 = AttentionDC(scale[3:7], channel, channel[-4])\n","        # self.csp = BottleneckCSP(channel[-1], channel[-1])\n","        self.rb1 = nn.Sequential(\n","            RB(channel[-1], channel[-1]),\n","            RB(channel[-1], channel[-1])\n","        )\n","        self.rb2 = nn.Sequential(\n","            RB(channel[-1]*2, channel[-2]),\n","            RB(channel[-2], channel[-2]),\n","        )\n","        self.rb3 = nn.Sequential(\n","            RB(channel[-2]*2, channel[-3]),\n","            RB(channel[-3], channel[-3]),\n","        )\n","        self.rb4 = nn.Sequential(\n","            RB(channel[-3]*2, channel[-4]),\n","            RB(channel[-4], channel[-4]),\n","        )\n","        self.rb5 = nn.Sequential(\n","            RB(channel[-4]*2, channel[-4]),\n","            RB(channel[-4], channel[-4])\n","        )\n","        self.rb6 = nn.Sequential(\n","            RB(channel[-4], channel[-4]),\n","            RB(channel[-4], channel[-4]),\n","        )\n","        self.head = nn.Conv2d(channel[-4], 1, 1)\n","\n","    \n","    def forward(self, x):\n","        fm1, fm2, fm3, fm4 = self.feature_extractor(x)\n","        decode1 = self.rb1(fm4) #328x7x7\n","        out1 = self.attention1([fm1, fm2, fm3, fm4, decode1]) #756x7x7\n","        decode2 = F.interpolate(self.rb2(out1), (out1.shape[2]*2, out1.shape[3]*2), mode=\"bilinear\") #192x14x14\n","        out2 = self.attention2([fm1, fm2, fm3, fm4, decode2]) #384x14x14\n","        decode3 = F.interpolate(self.rb3(out2), (out2.shape[2]*2, out2.shape[3]*2), mode=\"bilinear\") #80x28x28\n","        out3 = self.attention3([fm1, fm2, fm3, fm4, decode3]) #160x28x28\n","        decode4 = F.interpolate(self.rb4(out3), (out3.shape[2]*2, out3.shape[3]*2), mode=\"bilinear\") #56x56x56\n","        out4 = self.attention4([fm1, fm2, fm3, fm4, decode4]) #112x56x56\n","        decode5 = F.interpolate(self.rb5(out4), (out4.shape[2]*2, out4.shape[3]*2), mode=\"bilinear\") #32x112x112\n","        decode6 = F.interpolate(self.rb6(decode5), (decode5.shape[2]*2, decode5.shape[3]*2), mode=\"bilinear\")\n","        mask_pred = self.head(decode6)\n","        return mask_pred\n","\n","model = CTDC()\n","x = torch.rand(2,3,352,352)\n","out = model(x)\n","print(out.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"G_wL9JqC4vpY","papermill":{"duration":0.064284,"end_time":"2022-12-21T01:12:27.260520","exception":false,"start_time":"2022-12-21T01:12:27.196236","status":"completed"},"tags":[]},"source":["# loss"]},{"cell_type":"markdown","metadata":{"id":"BTOvVwldocvq","papermill":{"duration":0.061753,"end_time":"2022-12-21T01:12:27.385299","exception":false,"start_time":"2022-12-21T01:12:27.323546","status":"completed"},"tags":[]},"source":["## DiceLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:26.450892Z","iopub.status.busy":"2023-01-18T12:52:26.450424Z","iopub.status.idle":"2023-01-18T12:52:26.457971Z","shell.execute_reply":"2023-01-18T12:52:26.456934Z","shell.execute_reply.started":"2023-01-18T12:52:26.450854Z"},"id":"hpvkOGgI4vpY","papermill":{"duration":0.072665,"end_time":"2022-12-21T01:12:27.518753","exception":false,"start_time":"2022-12-21T01:12:27.446088","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class SoftDiceLoss(nn.Module):\n","    def __init__(self, smooth=1):\n","        super(SoftDiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, logits, targets):\n","        num = targets.size(0)\n","\n","        probs = torch.sigmoid(logits)\n","        m1 = probs.view(num, -1)\n","        m2 = targets.view(num, -1)\n","        intersection = m1 * m2\n","\n","        score = (\n","            2.0\n","            * (intersection.sum(1) + self.smooth)\n","            / (m1.sum(1) + m2.sum(1) + self.smooth)\n","        )\n","        score = 1 - score.sum() / num\n","        return score\n"]},{"cell_type":"markdown","metadata":{"id":"p2tfW_liogKr","papermill":{"duration":0.061463,"end_time":"2022-12-21T01:12:27.640549","exception":false,"start_time":"2022-12-21T01:12:27.579086","status":"completed"},"tags":[]},"source":["## FocalLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:26.460498Z","iopub.status.busy":"2023-01-18T12:52:26.459674Z","iopub.status.idle":"2023-01-18T12:52:26.473140Z","shell.execute_reply":"2023-01-18T12:52:26.472151Z","shell.execute_reply.started":"2023-01-18T12:52:26.460452Z"},"id":"OOKtqLSCoiz6","papermill":{"duration":0.07768,"end_time":"2022-12-21T01:12:27.780447","exception":false,"start_time":"2022-12-21T01:12:27.702767","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class FocalLoss(nn.Module):\n","    def __init__(self, gamma=0, alpha=None, size_average=True):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])\n","        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n","        self.size_average = size_average\n","\n","    def forward(self, input, target):\n","        if input.dim()>2:\n","            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n","            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n","            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n","        target = target.view(-1,1)\n","\n","        logpt = F.log_softmax(input)\n","        logpt = logpt.gather(1,target)\n","        logpt = logpt.view(-1)\n","        pt = Variable(logpt.data.exp())\n","\n","        if self.alpha is not None:\n","            if self.alpha.type()!=input.data.type():\n","                self.alpha = self.alpha.type_as(input.data)\n","            at = self.alpha.gather(0,target.data.view(-1))\n","            logpt = logpt * Variable(at)\n","\n","        loss = -1 * (1-pt)**self.gamma * logpt\n","        if self.size_average: return loss.mean()\n","        else: return loss.sum()"]},{"cell_type":"markdown","metadata":{"id":"cOYxiY2LomXm","papermill":{"duration":0.062357,"end_time":"2022-12-21T01:12:27.965242","exception":false,"start_time":"2022-12-21T01:12:27.902885","status":"completed"},"tags":[]},"source":["## Tversky-Kahneman"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:26.474931Z","iopub.status.busy":"2023-01-18T12:52:26.474434Z","iopub.status.idle":"2023-01-18T12:52:26.483859Z","shell.execute_reply":"2023-01-18T12:52:26.482926Z","shell.execute_reply.started":"2023-01-18T12:52:26.474893Z"},"id":"unWsAcGlok7g","papermill":{"duration":0.071545,"end_time":"2022-12-21T01:12:28.100423","exception":false,"start_time":"2022-12-21T01:12:28.028878","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# https://kornia.readthedocs.io/en/v0.1.2/losses.html"]},{"cell_type":"markdown","metadata":{"id":"Sw-r3tgnoX2L","papermill":{"duration":0.062321,"end_time":"2022-12-21T01:12:28.228522","exception":false,"start_time":"2022-12-21T01:12:28.166201","status":"completed"},"tags":[]},"source":["# Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:26.485777Z","iopub.status.busy":"2023-01-18T12:52:26.485261Z","iopub.status.idle":"2023-01-18T12:52:26.495576Z","shell.execute_reply":"2023-01-18T12:52:26.494646Z","shell.execute_reply.started":"2023-01-18T12:52:26.485741Z"},"id":"IzGKlHXL4vpZ","papermill":{"duration":0.07266,"end_time":"2022-12-21T01:12:28.365844","exception":false,"start_time":"2022-12-21T01:12:28.293184","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class DiceScore(torch.nn.Module):\n","    def __init__(self, smooth=1):\n","        super(DiceScore, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, logits, targets, sigmoid=True):\n","        num = targets.size(0)\n","\n","        probs = torch.sigmoid(logits)\n","        m1 = probs.view(num, -1) > 0.5\n","        m2 = targets.view(num, -1) > 0.5\n","        intersection = m1 * m2\n","\n","        score = (\n","            2.0\n","            * (intersection.sum(1) + self.smooth)\n","            / (m1.sum(1) + m2.sum(1) + self.smooth)\n","        )\n","        score = score.sum() / num\n","        return score\n"]},{"cell_type":"markdown","metadata":{"id":"0YslUp5c4vpZ","papermill":{"duration":0.061297,"end_time":"2022-12-21T01:12:28.488924","exception":false,"start_time":"2022-12-21T01:12:28.427627","status":"completed"},"tags":[]},"source":["# Config train"]},{"cell_type":"markdown","metadata":{"id":"SZWpgtT3c-px","papermill":{"duration":0.062646,"end_time":"2022-12-21T01:12:28.613021","exception":false,"start_time":"2022-12-21T01:12:28.550375","status":"completed"},"tags":[]},"source":["## Build"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:26.498903Z","iopub.status.busy":"2023-01-18T12:52:26.498652Z","iopub.status.idle":"2023-01-18T12:52:33.107545Z","shell.execute_reply":"2023-01-18T12:52:33.106060Z","shell.execute_reply.started":"2023-01-18T12:52:26.498879Z"},"id":"LPQ7Wxukc9xZ","outputId":"22c1ba9e-9402-4e4b-9ab1-06447fb72253","papermill":{"duration":5.929461,"end_time":"2022-12-21T01:12:34.605003","exception":false,"start_time":"2022-12-21T01:12:28.675542","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def build(args):\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","\n","    root_train = args.root + \"/TrainDataset\"\n","    root_test = args.root + \"/TestDataset\"\n","    train_dataloader, test_dataloader = get_dataloaders(\n","        root_train,\n","        root_test,\n","        batch_size=args.batch_size\n","    )\n","\n","    Dice_loss = SoftDiceLoss()\n","    BCE_loss = nn.BCELoss()\n","    TverskyLoss = tgm.losses.TverskyLoss(alpha=0.5, beta=0.5)\n","    FocalLoss = tgm.losses.FocalLoss(alpha=0.5, gamma=1, reduction='mean')\n","    Ssim = tgm.losses.SSIM(5, reduction='none')\n","    Smooth = tgm.losses.InverseDepthSmoothnessLoss()\n","    loss_fun = {'Dice_loss':Dice_loss, \"BCE_loss\":BCE_loss, \"TverskyLoss\":TverskyLoss, \"FocalLoss\":FocalLoss,\\\n","                \"Ssim\":Ssim, \"Smooth\":Smooth}\n","\n","    perf = DiceScore()\n","\n","    model = CTDC()\n","    if args.mgpu == \"true\":\n","        model = nn.DataParallel(model)\n","    model.to(device)\n","\n","    #===================== Optimizer ===================================================\n","    if args.optim == \"AdamW\":\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n","    elif args.optim == \"SGD\":\n","        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","    elif args.optim == \"Adadelta\":\n","        optimizer = torch.optim.Adadelta(model.parameters(), lr=args.lr)\n","    elif args.optim == \"Adagrad\":\n","        optimizer = torch.optim.Adagrad(model.parameters(), lr=args.lr)\n","    elif args.optim == \"Adam\":\n","        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","    elif args.optim == \"SparseAdam\":\n","        optimizer = torch.optim.SparseAdam(model.parameters(), lr=args.lr)\n","    elif args.optim == \"Adamax\":\n","        optimizer = torch.optim.Adamax(model.parameters(), lr=args.lr)\n","    elif args.optim == \"ASGD\":\n","        optimizer = torch.optim.Adamax(model.parameters(), lr=args.lr)\n","    elif args.optim == \"LBFGS\":\n","        optimizer = torch.optim.LBFGS(model.parameters(), lr=args.lr)\n","    elif args.optim == \"NAdam\":\n","        optimizer = torch.optim.NAdam(model.parameters(), lr=args.lr)\n","    elif args.optim == \"RAdam\":\n","        optimizer = torch.optim.RAdam(model.parameters(), lr=args.lr)\n","    elif args.optim == \"RMSprop\":\n","        optimizer = torch.optim.RMSprop(model.parameters(), lr=args.lr)\n","    elif args.optim == \"Rprop\":\n","        optimizer = torch.optim.Rprop(model.parameters(), lr=args.lr)\n","    #===================================================================================\n","\n","    if args.lrs == \"true\":\n","        if args.type_lr == \"LROnP\":\n","            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","                  optimizer, mode=\"max\", patience=10, factor=0.5, min_lr=args.lrs_min, verbose=True)\n","        elif args.type_lr == \"StepLR\":\n","            print(\"Using StepLR\")\n","            scheduler = torch.optim.lr_scheduler.StepLR(\n","                  optimizer, step_size=17, gamma=0.4, verbose=False)\n","        elif args.type_lr == \"MultiStepLR\":\n","            print(\"Using MultiStepLR\")\n","            scheduler = torch.optim.lr_scheduler.MultiStepLR(\n","                  optimizer, milestones=[10, 20, 30, 60], gamma=0.5, verbose=False)\n","\n","        \n","    if args.checkpoint_path == None:\n","        checkpoint = {\"test_measure_mean\":None, \"epoch\":0}\n","    else:\n","        checkpoint = torch.load(args.checkpoint_path)\n","        model.load_state_dict(checkpoint[\"model_state_dict\"])\n","        optimizer = torch.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","\n","    return (device, train_dataloader, test_dataloader, Dice_loss,\n","        BCE_loss, perf, model, optimizer, checkpoint, scheduler, loss_fun)\n","\n","( device, train_dataloader, test_dataloader, Dice_loss,\n","BCE_loss, perf, model, optimizer, checkpoint, scheduler, loss_fun) = build(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:33.115470Z","iopub.status.busy":"2023-01-18T12:52:33.112649Z","iopub.status.idle":"2023-01-18T12:52:33.127734Z","shell.execute_reply":"2023-01-18T12:52:33.126637Z","shell.execute_reply.started":"2023-01-18T12:52:33.115427Z"},"id":"skWEvnOdcdgP","outputId":"bfb6e451-cf80-43e2-b63d-726dcb015f16","papermill":{"duration":0.073559,"end_time":"2022-12-21T01:12:34.741570","exception":false,"start_time":"2022-12-21T01:12:34.668011","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:33.137290Z","iopub.status.busy":"2023-01-18T12:52:33.135028Z","iopub.status.idle":"2023-01-18T12:52:33.143318Z","shell.execute_reply":"2023-01-18T12:52:33.142326Z","shell.execute_reply.started":"2023-01-18T12:52:33.137254Z"},"id":"zMi6RoHT_SV7","papermill":{"duration":0.070192,"end_time":"2022-12-21T01:12:34.876565","exception":false,"start_time":"2022-12-21T01:12:34.806373","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-18T12:52:33.146157Z","iopub.status.busy":"2023-01-18T12:52:33.145425Z"},"id":"LMs20ZBv4vpZ","outputId":"ff79ceb8-bb55-456c-c0b9-3f251ce7e4b2","papermill":{"duration":33096.966842,"end_time":"2022-12-21T10:24:11.906876","exception":false,"start_time":"2022-12-21T01:12:34.940034","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def train_epoch(model, device, train_loader, optimizer, epoch, Dice_loss, BCE_loss):\n","    t = time.time()\n","    model.train()\n","    loss_accumulator = []\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        for k in range(0, data.shape[0], 4):\n","            data_input = data[k:k + 4]\n","            target_input = target[k:k+4]\n","            output = model(data_input)\n","            loss = Dice_loss(output, target_input) + BCE_loss(torch.sigmoid(output), target_input)\n","            loss.backward()\n","        optimizer.step()\n","        loss_accumulator.append(loss.item())\n","        if batch_idx + 1 < len(train_loader):\n","            print(\n","                \"\\rTrain Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}\\tTime: {:.6f}\".format(\n","                    epoch, (batch_idx + 1) * len(data), len(train_loader.dataset), 100.0 * (batch_idx + 1) / len(train_loader),\n","                    loss.item(), time.time() - t, ), end=\"\", )\n","        else:\n","            print(\n","                \"\\rTrain Epoch: {} [{}/{} ({:.1f}%)]\\tAverage loss: {:.6f}\\tTime: {:.6f}\".format(\n","                    epoch, (batch_idx + 1) * len(data), len(train_loader.dataset), 100.0 * (batch_idx + 1) / len(train_loader),\n","                    np.mean(loss_accumulator), time.time() - t, ) )\n","\n","    return np.mean(loss_accumulator)\n","\n","\n","@torch.no_grad()\n","def test(model, device, test_loader, epoch, perf_measure, phase, verbose=True):\n","    t = time.time()\n","    model.eval()\n","    perf_accumulator = []\n","    mIOU = []\n","    Dice = []\n","    for batch_idx, (data, target) in enumerate(test_loader):\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        perf_accumulator.append(perf_measure(output, target).item())\n","        mIOU.append(Fmstric.binary_jaccard_index(torch.sigmoid(output), target>0.5).item())\n","        Dice.append(torchmetrics.functional.dice(torch.sigmoid(output), target>0.5).item())\n","        if verbose:\n","            if batch_idx + 1 < len(test_loader):\n","                print(\n","                    \"\\r{}  Epoch: {} [{}/{} ({:.1f}%)]\\tDice: {:.6f}\\tmIOU: {:.6f}\\tDice: {:.6f}\\tTime: {:.6f}\".format(\n","                        phase, epoch, batch_idx + 1, len(test_loader), 100.0 * (batch_idx + 1) / len(test_loader),\n","                        np.mean(perf_accumulator), np.mean(mIOU), np.mean(Dice), time.time() - t, ), end=\"\", )\n","            else:\n","                print(\n","                    \"\\r{}  Epoch: {} [{}/{} ({:.1f}%)]\\tDice: {:.6f}\\tmIOU: {:.6f}\\tDice: {:.6f}\\tTime: {:.6f}\".format(\n","                        phase,epoch, batch_idx + 1, len(test_loader), 100.0 * (batch_idx + 1) / len(test_loader),\n","                        np.mean(perf_accumulator), np.mean(mIOU), np.mean(Dice), time.time() - t, ))\n","\n","    return np.mean(perf_accumulator), np.std(perf_accumulator), np.mean(mIOU)\n","\n","\n","\n","def train(args):\n","\n","    if not os.path.exists(\"./Trained models\"):\n","        os.makedirs(\"./Trained models\")\n","\n","    prev_best_test = checkpoint[\"test_measure_mean\"]\n","    test_measure_mean, test_measure_std = 0, 0\n","    print(\"best test:\", prev_best_test, \"epoch:\", checkpoint[\"epoch\"])\n","    \n","    for epoch in range(1, args.epochs + 1):\n","        Dices = [\"Dice\"]\n","        mIOUs = [\"mIOU\"]\n","        try:\n","            loss = train_epoch(\n","                model, device, train_dataloader, optimizer, epoch, loss_fun[\"Dice_loss\"], loss_fun[\"BCE_loss\"]\n","            )\n","            if epoch%2==0:\n","                for item in test_dataloader:\n","                    test_measure_mean, test_measure_std, miou = test(model, device, test_dataloader[item], epoch, perf, f\"Test {item}\", verbose=False)\n","                    Dices.append(test_measure_mean)\n","                    mIOUs.append(miou)\n","                t = Texttable()\n","                t.add_rows([['','Kvasir', 'ETIS', 'CVC-ColonDB', 'CVC-ClinicDB', \"CVC-T\"], Dices, mIOUs])\n","                print(t.draw())\n","            \n","        except KeyboardInterrupt:\n","            print(\"Training interrupted by user\")\n","            sys.exit(0)\n","        if args.lrs == \"true\":\n","            if args.type_lr == \"LROnP\":\n","                scheduler.step(test_measure_mean)\n","            else:\n","                scheduler.step()\n","        if prev_best_test == None or test_measure_mean > prev_best_test:\n","            print(\"Saving...\")\n","            torch.save(\n","                {\n","                    \"epoch\": epoch,\n","                    \"model_state_dict\": model.state_dict()\n","                    if args.mgpu == \"false\"\n","                    else model.module.state_dict(),\n","                    \"optimizer_state_dict\": optimizer.state_dict(),\n","                    \"scheduler\":scheduler.state_dict(),\n","                    \"test_measure_mean\": test_measure_mean,\n","                    \"test_measure_std\": test_measure_std,\n","                },\n","                f\"./Trained models/CTDCformer_epoch_backbonePvitB4_\" + args.dataset + \".pt\",\n","            )\n","            prev_best_test = test_measure_mean\n","\n","\n","def main():\n","    train(args)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRAea-7BNry1","papermill":{"duration":1.788313,"end_time":"2022-12-21T10:24:15.500438","exception":false,"start_time":"2022-12-21T10:24:13.712125","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def postprocess_image(image):\n","    predicted_map = np.array(image.detach().cpu())\n","    predicted_map = np.squeeze(predicted_map)\n","    predicted_map = predicted_map > 0\n","    return predicted_map\n","  \n","def saveImage(data, label, predict, path):\n","    plt.subplot(1,3,1)\n","    plt.imshow(data)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.subplot(1,3,2)\n","    plt.imshow(label, cmap=\"gray\")\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.subplot(1,3,3)\n","    plt.imshow(predict, cmap=\"gray\")\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.savefig(path)\n","\n","def predict():\n","    \n","    if not os.path.exists(\"./Predictions\"):\n","        os.makedirs(\"./Predictions\")\n","    if not os.path.exists(\"./Predictions/Trained on {}\".format(\"Kvar\")):\n","        os.makedirs(\"./Predictions/Trained on {}\".format(\"Kvar\"))\n","    \n","    t = time.time()\n","    model.eval()\n","    checkpoint = torch.load(\"./Trained models/CTDCformer_epoch_backbonePvitB4_Kvasir.pt\")\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    perf_accumulator = []\n","    mIOU = []\n","    Dice = []\n","    for batch_idx, (data, target) in enumerate(test_dataloader):\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        perf_accumulator.append(perf(output, target).item())\n","        mIOU.append(Fmstric.binary_jaccard_index(torch.sigmoid(output), target>0.5).item())\n","        Dice.append(torchmetrics.functional.dice(torch.sigmoid(output), target>0.5).item())\n","        input_image = data[0].permute(1,2,0).cpu().numpy()\n","        labels = postprocess_image(target)\n","        predicted_map = postprocess_image(output)\n","        saveImage(input_image, labels, predicted_map, \"./Predictions/Trained on {}/dice_{}_{}.jpg\".format(\n","                \"Kvar\",perf_accumulator[-1], batch_idx))\n","        # cv2.imwrite(\n","        #     \"/content/drive/MyDrive/Predictions/Trained on {}/dice_{}_{}.jpg\".format(\n","        #         \"Kvar\",perf_accumulator[-1], batch_idx),\n","        #     predicted_map * 255,\n","        # )\n","\n","        if batch_idx + 1 < len(test_dataloader):\n","            print(\n","                \"\\r{}  Epoch: {} [{}/{} ({:.1f}%)]\\tDice: {:.6f}\\tmIOU: {:.6f}\\tDice: {:.6f}\\tTime: {:.6f}\".format(\n","                    \"Predict\", 0, batch_idx + 1, len(test_dataloader), 100.0 * (batch_idx + 1) / len(test_dataloader),\n","                    np.mean(perf_accumulator), np.mean(mIOU), np.mean(Dice), time.time() - t, ), end=\"\", )\n","        else:\n","            print(\n","                \"\\r{}  Epoch: {} [{}/{} ({:.1f}%)]\\tDice: {:.6f}\\tmIOU: {:.6f}\\tDice: {:.6f}\\tTime: {:.6f}\".format(\n","                    \"Predict\",0, batch_idx + 1, len(test_dataloader), 100.0 * (batch_idx + 1) / len(test_dataloader),\n","                    np.mean(perf_accumulator), np.mean(mIOU), np.mean(Dice), time.time() - t, ))\n","\n","    return np.mean(perf_accumulator), np.std(perf_accumulator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJ1U-GgjFwFy","papermill":{"duration":1.526925,"end_time":"2022-12-21T10:24:18.662080","exception":false,"start_time":"2022-12-21T10:24:17.135155","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# predict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3z4CPcJUH_OL","papermill":{"duration":1.576701,"end_time":"2022-12-21T10:24:21.875499","exception":false,"start_time":"2022-12-21T10:24:20.298798","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# plt.subplot(1,3,1)\n","# plt.imshow(img1, cmap=\"gray\")\n","# plt.xticks([])\n","# plt.yticks([])\n","# plt.subplot(1,3,2)\n","# plt.imshow(img2, cmap=\"gray\")\n","# plt.xticks([])\n","# plt.yticks([])\n","# plt.subplot(1,3,3)\n","# plt.imshow(img3, cmap=\"gray\")\n","# plt.xticks([])\n","# plt.yticks([])\n","# plt.savefig(\"test.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgQMS-cXI8sE","papermill":{"duration":1.623893,"end_time":"2022-12-21T10:24:25.114582","exception":false,"start_time":"2022-12-21T10:24:23.490689","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"bentoml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"},"vscode":{"interpreter":{"hash":"d28dee128e23879305ee3a3236f31c2addbef97ecacc731e90c79949dc40e58b"}}},"nbformat":4,"nbformat_minor":4}
